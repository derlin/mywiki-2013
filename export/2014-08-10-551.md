---
title: "Using LVM"
date: "2014-08-10"
categories: 
  - "configuration-tricks"
  - "env"
---

## Display information

### Physical volumes

```bash
sudo pvdisplay
```

Wich results in this (quite queasy) output:
```
  --- Physical volume ---
PV Name               /dev/sda1
VG Name               sys
PV Size               111.79 GiB / not usable 4.00 MiB
Allocatable           yes
PE Size              4.00 MiB
Total PE              28617
Free PE               5944
Allocated PE          22673
PV UUID               c3cUMj-Snck-qebQ-yg0d-GgxE-Poje-cMdN6H

--- Physical volume ---
PV Name               /dev/md0
VG Name               raid
PV Size               8.19 TiB / not usable 5.00 MiB
Allocatable           yes
PE Size               4.00 MiB
Total PE              2146093
Free PE               954000
Allocated PE          1192093
PV UUID               1Y57gl-zeNd-xVzV-7y6N-Qk4n-d77Q-AI5F0p
```

To have a more useful info, like the space available on each PV, use `pvs`:
```bash
sudo pvs
sudo pvs --segments
sudo pvs /dev/sda1
```

Example of output:

```
[~] sudo pvs
PV         VG   Fmt  Attr PSize   PFree
/dev/md0   raid lvm2 a-- 8.19t  3.64t
/dev/sda1  sys  lvm2 a-- 111.79g 23.22g
```

### Logical volumes

Simply replace the “p” with “l” on the commands above, for example:

```bash
sudo lvdisplay
sudo lvs
```

----------------

## Snapshots

Snapshots are probably the most useful feature of lvm.

When creating a snapshot, you need to set the maximum space the latter can use. Note that it is just an upper limit (which can be modified latter if needed) and most of the time it won’t be reached since snapshots are _incremental_: a new snapshot takes 0b in space. After that, each modification of the lv will trigger a copy into the snapshot.

> A snapshot volume can be as large or a small as you like but it must be large enough to hold all the changes that are likely to happen to the original volume during the lifetime of the snapshot.

So, to create a snapshot named snap of the lv sys/root taking at most 10GB, use:

```bash
sudo lvcreate -s -n snap -L 10g sys/lroot
```

To later get back to the saved state, use:
```bash
sudo lvconvert --merge sys/snap
```

**__Note__**: If the origin volume of sys/snap is in use, it will inform you that the merge will take place the next time the volumes are activated. If this is the root volume, then at the next boot, the volume will be activated and the merge will begin in the background.

------------

## Resize logical volumes

Say that we have a lv named `raid/ldata` (raid is the volume group name) with an ext4 filesystem mounted on `/ldata` and mapped to `/dev/mapper/raid-ldata`. It’s size is currently 1.8 TB and we want to shrink it to 800 GB.
```bash
 > sudo lvs
  LV       VG   Attr      LSize  Pool Origin Data%  Move Log Copy%  Convert
  ldata    raid -wi-ao--- 1.82t
```

### 1. check and unmount the volume

All the operation should be done on an unmounted volume. If you want to resize the root partition, use a live CD.

It is important to check that the filesystem is not corrupt before proceeding. resizing a file system in an inconsistent state could have disastrous consequences! For extX fs, use:

```bash
sudo e2fsck -f /dev/mapper/raid-ldata
```

### 2. resize the filesystem

Shrink the fs to a size at most 90% of the new size you want. In our case, it will be ~700 GB:
```bash
sudo resize2fs /dev/mapper/raid-ldata 700G
```

> From the man page: The **resize2fs** program will resize ext2, ext3, or ext4 file systems. It can be used to enlarge or shrink an unmounted file system located on device. If the filesystem is mounted, it can be used to expand the size of the mounted filesystem, assuming the kernel supports on-line resizing.

### 3. reduce the size of the logical volume

```bash
sudo lvreduce -L 800G /dev/mapper/raid-ldata
```

result of the command:
```
WARNING: Reducing active logical volume to 800.00 GiB
  THIS MAY DESTROY YOUR DATA (filesystem etc.)
Do you really want to reduce ldata? [y/n]: y
  Reducing logical volume ldata to 800.00 GiB
  Logical volume ldata successfully resized
```

### 4. expand the fs to fit the whole partition

```bash
sudo resize2fs /dev/mapper/raid-ldata
```

Done!

result:
```bash
> sudo lvs
  LV       VG   Attr      LSize   Pool Origin Data%  Move Log Copy%  Convert
  ldata    raid -wi-a---- 800.00g
```

----------

# Troubleshooting

## Volume group not mapped/inactive at boot

### The issue

After upgrade, I could not boot into my system anymore, getting the grub error:

> ERROR: device ‘UUID=……’ not found. Skipping FSCK’ ERROR: Unable to find root device ‘UUID=……’ You are being dropped to the recovery shell Type ‘exit’ to try and continue booting sh: can’t access tty: job control turned off’

Using an archlinux live usb, I discovered that my lvm partitions did not show up in `/dev/mapper/`. First, the command `vgdisplay` showed not vg. Then, the command `lvdisplay` showed INACTIVE logical volumes…

### A Really partial solution

To activate a vg (and all the lv it contains), use the following:

```bash
vgscan
vgchange -ay
```

If you are in the _grub emergency shell_, the same commands **should be preceded by `lvm`**.

The system still doesn’t boot automatically, but running the above commands and then typing `exit` in the emergency shell works…
